# ChatRagi â€” AI-Powered Local RAG Chatbot

<p align="right">
  <img src="https://img.shields.io/badge/Python-3.8%2B-0074D9?style=for-the-badge&logo=python&logoColor=white" alt="Python Version" />
  <img src="https://img.shields.io/badge/Flask-Web%20App-000000?style=for-the-badge&logo=flask&logoColor=white" alt="Flask Web App" />
  <img src="https://img.shields.io/badge/Ollama-Local%20LLMs-39CCCC?style=for-the-badge&logo=code&logoColor=white" alt="Ollama Local LLMs" />
  <img src="https://img.shields.io/badge/ChromaDB-Vector%20Search-6610F2?style=for-the-badge&logoColor=white" alt="ChromaDB Vector Search" />
</p>
<p align="right">
  <img src="https://img.shields.io/badge/v1.1.0%20â€“%20Pre--Release-FFA500?style=for-the-badge&logo=semantic-release&logoColor=white" alt="Version v1.1.0 â€“ Pre-Release" />
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge&logo=open-source-initiative&logoColor=white" alt="MIT License" />
  <img src="https://img.shields.io/badge/Build-Passing-brightgreen?style=for-the-badge&logo=githubactions&logoColor=white" alt="Build Status: Passing" />
</p>

**ChatRagi** is a fully local **Retrieval-Augmented Generation (RAG)** chatbot framework that integrates local LLMs (via Ollama), ChromaDB for vector search, and a lightweight Flask web app.

It delivers context-aware, memory-augmented responses based on your indexed documents â€” entirely private, running on your own machine.

---

## Features

- **Document Ingestion**: Parses and indexes PDFs, CSVs, TXT, JSON, and Markdown files into ChromaDB for semantic search.
- **Local Chat Interface**: Chat naturally with your documents through a fast, responsive web UI powered by HTML, CSS, and JavaScript.
- **Embeddings + Retrieval**: Leverages local embedding models (e.g., `nomic-embed-text`) for meaningful, context-rich retrieval.
- **Persona Tone Support**: Choose how ChatRagi responds â€” ðŸ§  Neutral, ðŸ’¼ Professional, or ðŸ˜„ Witty â€” dynamically from the chat interface.
- **Memory-Aware Conversations**: Recalls relevant previous interactions to enrich answers where needed.
- **Fully Modular Design**: Clean separation of ingestion, memory, chatbot logic, and backend services for easy customization.
- **100% Local Execution**: All document processing, embeddings, vector storage, and LLM answering happen locally â€” no cloud dependencies.

---

## ChatRagi System Overview

ChatRagi is a fully local Retrieval-Augmented Generation (RAG) system that allows you to chat with your own documents in natural language â€” powered entirely by local models and vector search.

The diagram below shows the core components of the system, from document ingestion and semantic indexing to memory-aware querying and tone-controlled response generation.

![Flow diagram showing ChatRagi's document ingestion, vector embedding using nomic-embed-text, storage in ChromaDB, query processing, memory retrieval, and answer generation using a local LLM (e.g., phi4) â€” all running fully offline.](docs/images/Chatragi-Overview.png)

> **Diagram**: End-to-end architecture of ChatRagi's local RAG pipeline â€” from document ingestion and embedding to semantic search, prompt construction, and LLM-powered response generation.

---

### How It Works

#### Part 1: Document Ingestion & Indexing

1. **Drop files into `/data/`**  
   - Add PDFs, JSON, TXT, CSV, or Markdown files to the `data/` folder.  
   - ChatRagi continuously monitors this folder and detects new files in real-time.
2. **Chunk, Embed, Store, and Archive**  
   - Each file is parsed and split into smaller, meaningful chunks.  
   - These chunks are embedded into 768-dimensional vectors using a local model (e.g., `nomic-embed-text` via Ollama).  
   - The resulting vectors are stored in **ChromaDB**, the local vector database.  
   - Once processed, the original files are automatically moved to the `archive/` folder to prevent duplicate ingestion and keep your workspace clean.

#### Part 2: Querying the Chatbot & Generating Answers

3. **User submits a query**  
   - The question is converted into an embedding and used to semantically search ChromaDB for the most relevant document chunks.
4. **Context + Persona = Smart Prompt**  
   - Relevant past memories (if available) and top-matching document chunks are combined with the current query.  
   - If a response tone is selected â€” ðŸ§  Neutral, ðŸ’¼ Professional, or ðŸ˜„ Witty â€” it's applied here before sending the prompt to the LLM.
5. **Answer generated by local LLM**  
   - The full prompt is passed to a local language model (e.g., `phi4`) via Ollama.  
   - The LLM generates a clean, context-aware Markdown response, which is rendered directly in the Web UI.

> âš¡ Everything runs fully offline â€” no cloud APIs, no data leakage, and complete control over your knowledge base.

---

## Quick Links

> âš¡ Refer to [Running LLMs Locally](docs/Running-LLMs-Locally-README.md) for instructions on configuring `phi4`, `nomic-embed-text`, or a custom model of your choice.  
> ðŸ“– See the [Architecture Overview](docs/architecture.md) for system diagrams and design details.  
> ðŸž Review [Known Issues](KNOWN_ISSUES.md) for limitations and open bugs.  
> âœï¸ Setup instructions are in the [Setup Instructions](#setup-instructions) section below.

---

## Project Structure

```bash
chatragi/
â”œâ”€â”€ src/                      # Core Python application logic
â”‚   â””â”€â”€ chatragi/         
â”‚       â”œâ”€â”€ app.py             # Main entry point to run the Flask app
â”‚       â”œâ”€â”€ config.py          # Global settings (paths, model names, etc.)
â”‚       â”œâ”€â”€ file_watcher.py    # Watches for new documents added to /data/
â”‚       â”œâ”€â”€ cli.py             # Command Line Interface for ChatRagi (optional)
â”‚       â”œâ”€â”€ utils/             # Modular utilities and helper functions
â”‚       â”‚   â”œâ”€â”€ chat_memory.py
â”‚       â”‚   â”œâ”€â”€ chatbot.py
â”‚       â”‚   â”œâ”€â”€ db_utils.py
â”‚       â”‚   â”œâ”€â”€ document_loader.py
â”‚       â”‚   â”œâ”€â”€ error_handler.py
â”‚       â”‚   â”œâ”€â”€ logger_config.py
â”‚       â”‚   â””â”€â”€ persona.py
â”‚       â”œâ”€â”€ templates/         # HTML templates for Flask Web UI      
â”‚       â”‚   â””â”€â”€ index.html        
â”‚       â””â”€â”€ static/            # Frontend static assets (CSS, JS)
â”‚           â”œâ”€â”€ css/
â”‚           â”‚   â””â”€â”€ styles.css
â”‚           â””â”€â”€ js/
â”‚               â””â”€â”€ scripts.js
â”œâ”€â”€ chroma_db/                 # âœ¨ Auto-generated: Local ChromaDB vector store
â”œâ”€â”€ data/                      # âœ¨ Auto-generated: Input files for ingestion
â”œâ”€â”€ archive/                   # âœ¨ Auto-generated: Processed files archive
â”œâ”€â”€ docs/                      # Guides, architecture diagrams, and setup instructions
â”œâ”€â”€ notebooks/                 # Development and testing notebooks
â”œâ”€â”€ logs/                      # âœ¨ Auto-generated: Runtime logs
â”œâ”€â”€ storage/                   # âœ¨ Auto-generated: Output and chatbot memory storage
â”œâ”€â”€ test/                      # Unit and integration tests
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md/
â”œâ”€â”€ CHANGELOG.md
â””â”€â”€ LICENSE

# âœ¨ Note: Folders like /chroma_db, /data, /archive, /logs, and /storage
# are auto-created the first time you run the app.
```

---

## Setup Instructions

### Prerequisites:
Before you begin, make sure the following are installed on your system:

| **Tool**             | **Recommended Version / Notes**                                                      |
| -------------------- | ------------------------------------------------------------------------------------ |
| **Python**           | 3.11.8 or higher (64-bit)                                                            |
| **IDE / Editor**     | Visual Studio Code (or equivalent)                                                   |
| **Terminal / Shell** | Bash, PowerShell, or zsh (virtualenv support)                                        |
| **Ollama**           | Must be installed and running locally ([Guide](docs/Running-LLMs-Locally-README.md)) |
| **Git** (optional)   | For cloning and managing the repository                                              |

> ðŸ’¡ You can verify Python installation using `python --version` or `python3 --version`.  
> ðŸ› ï¸ You can also use tools like `pyenv` to manage multiple Python versions.  

### Development Environment Used:

> ðŸ–¥ï¸ **Mac Studio (2022)** â€” Apple M1 Max, 32â€¯GB RAM  
> ðŸ› ï¸ **IDE**: Visual Studio Code  
> ðŸ§  **Optimized for local LLMs via Ollama**, with:
> - Fast embeddings using `nomic-embed-text` (768-dim)
> - Low-latency responses via `phi4`
> - No Docker GPU acceleration (Apple Silicon limitation), so tuned for efficient context handling and minimal I/O overhead

> ðŸ’¡ **Performance Tip**  
>> If you're running on a lower-spec device, try a smaller model like: `llama3.2:3b`. This loads faster and lets you test functionality before switching to larger models like `phi4:14b`. 
>> See [Running LLMs Locally](docs/Running-LLMs-Locally-README.md) and [Configuration Overview](Config-README.md) for detailed setup instructions.

---

### 1. Clone the repository and create a virtual environment

```bash
git clone https://github.com/sssethi-dg/chatragi.git
cd chatragi

# Create a virtual environment
python3 -m venv .venv

# ðŸ’¡ Tip: On Windows, use `python` instead of `python3` if you encounter an error
python -m venv .venv
```

### 2. Activate the virtual environment

**On macOS / Linux:**
```bash
source .venv/bin/activate
```

**On Windows (CMD or PowerShell):**
```bash
.\venv\Scripts\activate
```
> âš ï¸ PowerShell Users:
> If you encounter a script execution error, run the following command first:
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```
**If activation still fails:**
```powershell
# Manually navigate to the Scripts directory
cd .venv\Scripts
.\activate
cd ../..  # Return to project root
```
> ðŸ’¡ Pro Tip for Git Bash users:
> The activation syntax is slightly different:
```bash
source .venv/Scripts/activate
```

### 3. Upgrade pip and install dependencies

```bash
python -m pip install --upgrade pip setuptools wheel

# Install project in editable (development) mode
pip install -e .

# (Optional) Install developer tools
pip install -e '.[dev]'
```

### Windows Users â€” Troubleshooting Chroma Installation

```bash
ERROR: Failed to build installable wheels for some pyproject.toml based projects (chroma-hnswlib)
```

It usually means C++ build tools are missing. To fix it:
1.	Download and install the [Visual C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/).
2.	During installation, select the following Individual Components:
	â€¢	âœ… MSVC v143 â€“ VS 2022 C++ x64/x86 build tools (Latest)
	â€¢	âœ… Windows 11 SDK â€“ Version 10.0.22621.0 or latest available
3.	After installation, re-run the pip install commands.

---
## Part 1: Document Ingestion Service

ChatRagi includes a lightweight file watcher that continuously monitors the `data/` folder for new documents.

When a supported file is added, itâ€™s automatically parsed, embedded, and indexed into ChromaDB â€” no manual steps required.

### Start the service

```shell
python src/chatragi/file_watcher.py
```

### Add new files

Once the service is running, copy or move your documents into the data/ folder:
```bash
# Move or copy your file into the data/ folder
cp /path/to/my-notes.pdf data/
```

> Supported formats: .pdf, .csv, .txt, .json, .md

ChatRagi will detect the new file, process it in real-time, and log the output.

### Sample output

```text
INFO - ChatRagi - Successfully connected to ChromaDB!
INFO - ChatRagi - Starting File Watcher Service...
INFO - ChatRagi - Watching ".../chatragi/data" for new files...
INFO - ChatRagi - Detected new file: my-notes.pdf
INFO - ChatRagi - Processed and indexed: my-notes.pdf
INFO - ChatRagi - Moved file to archive/
```

### How It works

![Flow diagram showing ChatRagi's document ingestion process â€” files are dropped into a watched folder, chunked and embedded into vectors via nomic-embed-text, stored in ChromaDB, and moved to an archive folder after successful indexing.](docs/images/Chatragi-Data-Ingestion.png)

> **Diagram**: ChatRagiâ€™s real-time document ingestion pipeline â€” monitors the `/data/` folder, chunks and embeds content using a local model, stores vectors in ChromaDB, and archives the original files after processing.

1. **Monitor the data folder**  
   The file watcher continuously observes the `data/` folder for new files in real time.
2. **Validate file readiness**  
   It checks that each file is stable (not still being written) and skips hidden/system files like `.DS_Store`.
3. **Parse and chunk the content**  
   Once validated, documents are parsed and broken into context-aware chunks optimized for embedding.
4. **Generate vector embeddings**  
   Each chunk is passed to the local `nomic-embed-text` model (via Ollama) and transformed into a 768-dimensional vector.
5. **Store and archive**  
   Vectors are stored in ChromaDB, and the original file is moved to the `archive/` folder to prevent re-processing.

> ðŸ›‘ To stop the service, press Ctrl + C.

For additional details, please refer to **[File Watcher](docs/File-Watcher-README.md)** README file.

---

##  Part 2: Web Chatbot (Flask App)

ChatRagi includes a local web-based chatbot powered by Flask that lets you chat with your documents through a lightweight UI â€” no login or cloud setup required.

![Flow diagram showing how ChatRagi processes a user query â€” the question is embedded using nomic-embed-text, matched against ChromaDB, and the most relevant chunks are passed to a local LLM via Ollama to generate a response.](docs/images/Chatragi-WebChatbot.png)

> **Diagram**: ChatRagiâ€™s query handling pipeline â€” takes user input, embeds the query using nomic-embed-text, retrieves relevant vectors from ChromaDB, and passes context to a local LLM (like phi4) to generate a Markdown response in the Web UI.

### Start the web app

Run the following command from your project root:
```shell
python src/chatragi/app.py
```

### Sample terminal output

```text
INFO - ChatRagi - Refreshing index...
INFO - ChatRagi - Index is ready.
* Running on http://127.0.0.1:5000
```

### Access the chat interface

Open your browser and go to:
[http://127.0.0.1:5000](http://127.0.0.1:5000)

> ðŸ›‘ To stop the Chatbot App, press **Ctrl + C**.

For additional details, please refer to **[App](docs/App-README.md)** README file.

---

## Using the ChatRagi Web App

Once the app is running, you'll see the ChatRagi interface in your browser.

![Screenshot of the ChatRagi Web UI showing the chatbot interface with a query input box, tone selector dropdown, and action buttons for memory, sources, index refresh, and clear.](docs/images/ChatRagi-WebPage.png)

> **Screenshot**: The ChatRagi Web App interface â€” includes a tone selector, input field, send button, and interactive controls to mark important responses, download chat, view memory, list sources, refresh the index, and clear the chat window.

### Web App Controls

**Top Right Buttons**
- **Mark as Important** â€” Flags the latest Q&A for future reference.
- **Download Chat** â€” Saves the chat session to a `.txt` file.
- **Toggle Dark Mode** â€” Switch between light and dark UI themes.

**Bottom Action Buttons**
- **Show Saved Memory** â€” View your past questions and answers.
- **List Sources** â€” See all documents that have been indexed.
- **Refresh Index** â€” Re-ingest files from the data/ folder.
- **Clear** â€” Clears the current conversation and input field.

### Ask a question

- Enter your question in the input box.
- Click the **Send** button (ðŸ“¤).
- ChatRagi will search your documents and return a structured answer.

### Select Response Tone

Use the **Select Tone** dropdown to customize how ChatRagi responds:

- ðŸ§  **Neutral** â€” Balanced, clear, and helpful.
- ðŸ’¼ **Professional** â€” Formal, structured, and business-friendly.
- ðŸ˜„ **Witty** â€” Playful, clever, and conversational.

> Your tone choice affects the phrasing, not the facts.
> The underlying response saved to memory is always the neutral version.

ðŸ’¡ **Tip**: Try asking the same question in different tones â€” the style shifts, but the meaning remains consistent.

### What Happens Behind the Scenes

1. You submit a natural language query.
2. The query is embedded using `nomic-embed-text` (768-dim vector via Ollama).
3. ChromaDB is searched for semantically similar chunks.
4. Relevant chunks and any memory context are merged to form a structured prompt.
5. The local LLM (e.g., `phi4`) generates a Markdown-formatted response.
6. The response is returned and displayed in the UI.

> ðŸ’¡ If sources are cited, filenames will appear beneath the response â€” ensuring transparency.

---

## Contribution Status

> âš ï¸ This repository is currently **read-only**.  
> Contributions, issues, and pull requests are disabled until a future release.  
> Feel free to clone, explore, and use the project locally!  

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---

## Maintainer

Developed and maintained by [Simer Singh Sethi](mailto:simer@disruptivegeek.net)

If you find **ChatRagi** helpful and want to support continued development:

[![Buy Me a Coffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-FF813F?style=for-the-badge&logo=buymeacoffee&logoColor=white)](https://buymeacoffee.com/sssethi)

Your support helps keep the dev caffeine flowing!